{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMdnGj4KENQSOqxxLJ1g4Zf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fbeilstein/dbms/blob/master/DB_lecture_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distributed Transactions**\n",
        "\n",
        "To maintain order in a distributed system, we have to guarantee at least some consistency. \n",
        "\n",
        "We already talked about single-object, single-operation consistency models but in databases we **often need to execute multiple operations atomically**.\n",
        "\n",
        "\n",
        "Generally speaking, when transferring money from one account to another, you’d like to both credit the first account and debit the second one simultaneously. **Note:** it can be that credit and debit are storen in differeent DBs of parts of one DB. However, if we break down the transaction into individual steps, even debiting or crediting doesn’t look atomic at first sight: we need to read the old balance, add or subtract the required amount, and save this result. Each one of these substeps involves several operations: the node receives a request, parses it, locates the data on disk, makes a write and, finally, acknowledges it. Even this is a rather high-level view: to execute a simple write, we have to perform hundreds of small steps. This means that we have to first execute the transaction and only then make its results visible. \n",
        "\n",
        "\n",
        "A **transaction** is a set of operations, an atomic unit of execution. Transaction **atomicity** implies that all its results become visible or none of them do. \n",
        "\n",
        "For example, if we modify several rows, or even tables in a single transaction, either all or none of the modifications will be applied. To ensure atomicity, transactions should be **recoverable**. In other words, if the transaction cannot complete, is aborted, or times out, its results have to be rolled back completely. A nonrecoverable, partially executed transaction can leave the database in an inconsistent state. \n",
        "\n",
        "In case of unsuccessful transaction execution, the database state has to be reverted to its previous state, as if this transaction was never tried in the first place.\n",
        "\n",
        "\n",
        "\n",
        "Another important aspect is network partitions and node failures: nodes in the system fail and recover independently, but their states have to remain consistent. This means that the atomicity requirement holds not only for the local operations, but also for operations executed on other nodes: changes have to be durably propagated to all of the nodes involved in the transaction or none of them.\n",
        "\n"
      ],
      "metadata": {
        "id": "f6SxvdgwUazg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transactions apply operations to data records starting at some point in time. This gives us some flexibility in terms of scheduling and execution: transactions can be reordered and even retried.\n",
        "\n",
        "\n",
        "The main focus of transaction processing is to determine permissible histories, to model and represent possible interleaving execution scenarios.  History represents a dependency graph: which transactions have been executed prior to execution of the current transaction. History is said to be serializable if it is equivalent (i.e., has the same dependency graph) to some history that executes these transactions sequentially. \n",
        "\n",
        "\n",
        "Single-partition transactions involve the pessimistic (lock-based or tracking) or optimistic (try and validate) concurrency control schemes, but neither one of these approaches solves the problem of multipartition transactions, which require coordination between different servers, distributed commit, and rollback protocols.\n"
      ],
      "metadata": {
        "id": "HvU4RYsFXsqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Making Operations Appear Atomic**\n",
        "\n",
        "\n",
        "To make multiple operations **appear atomic**, especially if some of them are remote, we need to use a class of algorithms called **atomic commitment**. \n",
        "\n",
        "Atomic commitment needs **consensus** between the participants: a transaction will not commit if even one of the participants votes against it. \n",
        "\n",
        "At the same time, this means that failed processes have to reach the same conclusion as the rest of the cohort. Another important implication of this fact is that atomic commitment algorithms do not work in the presence of Byzantine failures: when the process lies about its state or decides on an\n",
        "arbitrary value, since it contradicts unanimity.\n",
        "\n",
        "\n",
        "**The problem** = to be or not to be, commit or do not commit, **cannot change transaction itself**\n",
        "\n",
        "Database implementers have to decide on:\n",
        "* When the data is considered ready to commit, and they’re just a pointer swap\n",
        "away from making the changes public.\n",
        "* How to perform the commit itself to make transaction results visible in the shortest timeframe possible.\n",
        "* How to roll back the changes made by the transaction if the algorithm decides\n",
        "not to commit.\n",
        "\n",
        "Many distributed systems use atomic commitment algorithms: MySQL (for distributed transactions), Kafka (for producer and consumer interaction),etc.\n",
        "\n",
        "\n",
        "The **transaction manager** is a subsystem responsible for scheduling, coordinating, executing, and tracking transactions. In a distributed\n",
        "environment, the transaction manager is responsible for ensuring that node-local visibility guarantees are consistent with the visibility prescribed by distributed atomic operations = transactions commit in all partitions, and for all replicas."
      ],
      "metadata": {
        "id": "vHg7thTBYCfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Two-Phase Commit**\n",
        "\n",
        "2PC executes in two phases. During the first phase, the decided value is distributed, and votes are collected. During the second phase, nodes just flip the switch, making the results of the first phase visible.\n",
        "\n",
        "\n",
        "2PC assumes the presence of a **leader** (coordinator) that holds the state, collects votes, and is a primary point of reference for the agreement round. The rest of the nodes are called **cohorts** -- usually partitions that operate over disjoint datasets. \n",
        "\n",
        "\n",
        "The coordinator can be \n",
        "* node that received a request to execute the transaction\n",
        "* picked at random\n",
        "* by leader-election algorithm\n",
        "* assigned manually\n",
        "* fixed throughout the lifetime of the system\n",
        "* transferred to another participant for reliability or performance\n",
        "\n",
        "\n",
        "**Execution**\n",
        "* **Prepare** The coordinator notifies cohorts about the new transaction by sending a **propose message**. Cohorts make a decision on whether or not they can commit the part of the transaction that applies to them. Then they send coordinator the vote \"commit/abort\".\n",
        "* **Commit/abort** Operations within a transaction can change state across different partitions (each represented by a cohort). If even 1 votes for abort -> abort all message. If all commit -> commit all message.\n",
        "\n",
        "\n",
        "During each step the coordinator and cohorts have to write the results of each operation to durable storage to be able to reconstruct the state and recover in case of local failures, and be able to forward and replay results for other participants.\n",
        "\n",
        "\n",
        "In the context of database systems, each 2PC round is usually responsible for a single transaction. During the prepare phase, transaction contents (operations, identifiers, and other metadata) are transferred from the coordinator to the cohorts. The transaction is executed by the cohorts locally and is left in a **partially committed state** (sometimes called **precommitted**), making it ready for the coordinator to finalize execution during the next phase by either committing or aborting it. By the time the transaction commits, its contents are already stored durably on all other nodes."
      ],
      "metadata": {
        "id": "qazEnPISz7fJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cohort Failures in 2PC**\n",
        "\n",
        "Cohort fails -> commit should be aborted.\n",
        "\n",
        "This requirement has a negative impact on availability. Spanner and etc perform 2PC over Paxos groups rather than individual nodes to improve protocol availability.\n",
        "\n",
        "\n",
        "If one of the cohorts has failed after accepting the proposal, it has to learn about the actual outcome of the vote before it can serve values correctly, since the coordinator might have aborted the commit due to the other cohorts’ decisions. \n",
        "\n",
        "cohort -> accept commit -> fail -> recovery -> request decision log from coordinator -> commit or reject\n",
        "\n",
        "\n",
        "Link failures might lead to message loss, and this wait will continue indefinitely. If the coordinator does not receive a response from the replica during the propose phase, it can trigger a timeout and abort the transaction"
      ],
      "metadata": {
        "id": "ApC0lw-dItga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Coordinator Failures in 2PC**\n",
        "\n",
        "\n",
        "* coordinator made decision, but link to particular node failed -> node requests decision from peers. Replicating commit decisions is safe since it’s always unanimous: the whole point of 2PC is to either commit or abort on all sites, and commit on one cohort implies that all other cohorts have to commit.\n",
        "* coordinator collects votes and fails -> wait for coordinator recovery or choose new coordinator and revote\n",
        "\n",
        "Many databases use 2PC: MySQL, PostgreSQL, MongoDB, etc. \n",
        "\n",
        "\n",
        "(+) simple (easy to reason about, implement, and debug)\n",
        "\n",
        "(+) low overhead (message complexity and the number of round-trips of the protocol are low)\n",
        "\n",
        "(-) needs proper recovery mechanisms\n",
        "\n",
        "(-) A two-phase commit protocol cannot dependably recover from a failure of **both** the coordinator and a cohort member during the Commit phase. If both the coordinator and a cohort member failed, it is possible that the failed cohort member was the first to be notified, and had actually done the commit."
      ],
      "metadata": {
        "id": "5cmnyOtoKtPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Three-Phase Commit**\n",
        "\n",
        "In a situation where coordinator fails, remaining sites are bound to first select new coordinator. This new coordinator checks status of the protocol from the remaining sites. If the coordinator had decided to commit, at least one of other ‘k’ sites that it informed will be up and will ensure that commit decision is respected. The new coordinator restarts third phase of protocol if any of rest sites knew that old coordinator intended to commit transaction. Otherwise, new coordinator aborts the transaction.\n",
        "\n",
        "\n",
        "The three-phase commit (3PC) protocol adds an extra step, and timeouts on both sides that can allow cohorts to proceed with either commit or abort in the event of coordinator failure, depending on the system state. \n",
        "\n",
        "3PC assumes a synchronous model and that communication failures are not possible.\n",
        "\n",
        "\n",
        "* **Propose** The coordinator sends out a proposed value and collects the votes.\n",
        "* **Prepare** The coordinator notifies cohorts about the vote results. If the vote has passed and all cohorts have decided to commit, the coordinator sends a Prepare message, instructing them to prepare to commit. Otherwise, an Abort message is sent and the round completes.\n",
        "* **Commit** Cohorts are notified by the coordinator to commit the transaction.\n",
        "\n",
        "\n",
        "Propose phase crash or timeout (coordinator or cohort) -> abort transaction.\n",
        "\n",
        "Prepare phase crash or timeout (coordinator or cohort) -> abort transaction.\n",
        "\n",
        "Commit phase crash or timeout (coordinator or cohort) -> commit transaction."
      ],
      "metadata": {
        "id": "Ep6YgNKJOIu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Coordinator Failures in 3PC**\n",
        "\n",
        "\n",
        "All state transitions are coordinated, and cohorts can’t move on to the next phase until everyone is done with the previous one: the coordinator has to wait for the replicas to continue. Cohorts can eventually abort the transaction if they do not hear from the coordinator before the timeout, if they didn’t move past the prepare phase.\n",
        "\n",
        "\n",
        "As we discussed previously, 2PC cannot recover from coordinator failures, and\n",
        "cohorts may get stuck in a nondeterministic state until the coordinator comes back. 3PC avoids blocking the processes in this case and allows cohorts to proceed with a deterministic decision.\n",
        "\n",
        "\n",
        "The worst-case scenario for the 3PC is a network partition: some nodes successfully move to the prepared state, and now can proceed with commit after the timeout. Some can’t communicate with the coordinator, and will abort\n",
        "after the timeout. This results in a split brain: some nodes proceed with a commit and some abort, all according to the protocol, leaving participants in an inconsistent and contradictory state\n",
        "\n",
        "(+) particularly solves the problem with 2PC blocking\n",
        "\n",
        "(-) larger message overhead\n",
        "\n",
        "(-) introduces potential contradictions\n",
        "\n",
        "(-) does not work well in the presence of network partitions. This might be the primary reason 3PC is not widely used in practice."
      ],
      "metadata": {
        "id": "u2R8gwDub5sa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distributed Transactions with Calvin**\n",
        "\n",
        "\n",
        "\n",
        "Problem: reduce the total amount of time during which transactions hold locks.\n",
        "\n",
        "One of the ways to do this is to let replicas agree on the execution order and transaction boundaries before acquiring locks and proceeding with execution - failed node may recover from other participants that execute the same transaction in parallel.  \n",
        "\n",
        "traditional DB | [Calvin](https://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf)\n",
        "---|---\n",
        "two-phase locking or optimistic concurrency control | deterministic order = uses a sequencer (an entry point for all transactions) \n",
        "| since all replicas get the same inputs, they also produce equivalent outputs.\n",
        "no deterministic transaction order | deterministic transaction order\n",
        "MySQL | FaunaDB\n",
        "\n",
        "one partition: sequencer -> scheduler -> worker -> storage\n",
        "\n",
        "**sequencer**\n",
        "* The sequencer determines the order in which transactions are executed, and\n",
        "establishes a global transaction input sequence. \n",
        "* To minimize contention and batch decisions, the timeline is split into epochs. \n",
        "* The sequencer collects transactions and groups them into short time windows = replication units, so transactions do not have to be communicated separately.\n",
        "* As soon as a transaction batch is successfully replicated, sequencer forwards it to the scheduler, which orchestrates transaction execution.\n",
        "\n",
        "**scheduler**\n",
        "* uses a deterministic scheduling protocol that executes parts of transaction in parallel, while preserving the serial execution order specified by the sequencer. \n",
        "* applying transaction to a specific state is guaranteed to produce only changes specified by the transaction and transaction order is predetermined, replicas do not have to further communicate with the sequencer.\n",
        "* each transaction in Calvin has a **read set** and a **write set**. Calvin does not natively support transactions that rely on additional reads that would determine read and write sets.\n",
        "\n",
        "\n",
        "**worker thread**\n",
        "* It analyzes the transaction’s read and write sets, determines node-local data\n",
        "records from the read set, and creates the list of active participants (i.e., ones that hold the elements of the write set, and will perform modifications on the data).\n",
        "* It collects the local data required to execute the transaction, in other words, the read set records that happen to reside on that node. The collected data records are forwarded to the corresponding active participants.\n",
        "* If this worker thread is executing on an active participant node, it receives data records forwarded from the other participants, as a counterpart of the operations executed during step 2.\n",
        "* Finally, it executes a batch of transactions, persisting results into local storage. It does not have to forward execution results to the other nodes, as they receive the same inputs for transactions and execute and persist results locally themselves.\n",
        "\n",
        "\n",
        "A typical Calvin implementation colocates sequencer, scheduler, worker, and storage subsystems. Conses: Paxos algorithm (next lecture, no time) or leader replica.\n"
      ],
      "metadata": {
        "id": "Vo3vJGf1szHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distributed Transactions with Spanner**\n",
        "\n",
        "\n",
        "Calvin is often contrasted with another approach for distributed transaction management called Spanner (CockroachDB, YugaByte DB, etc). \n",
        "\n",
        "* uses two-phase commit over consensus groups per partition (shard)\n",
        "* TrueTime to achieve consistency and impose transaction order\n",
        "\n",
        "TrueTime = a high-precision wall-clock API that also exposes an uncertainty bound, allowing local operations to introduce artificial slowdowns to wait for the uncertainty bound to pass.\n",
        "\n",
        "Spanner offers three main operation types: \n",
        "* **read-write transactions** require locks, pessimistic concurrency control, and presence of the leader replica\n",
        "* **read-only transactions** lock-free and can be executed at any replica\n",
        "* **snapshot reads** leader is required for the latest timestamp reads\n",
        "\n",
        "Reads at the specific timestamp are consistent, since values are versioned and snapshot contents can’t be changed once written. Each data record has a timestamp assigned, which holds a value of the transaction commit time. This also implies that multiple timestamped versions of the record can be stored.\n",
        "\n",
        "\n",
        "Spanner architecture is too complicated to discuss in details. \n",
        "\n",
        "* Each **replica** holds several **tablets**, with **Paxos state machines** attached to them. Replicas are grouped into replica sets called Paxos groups -- a unit of data placement and replication. \n",
        "* Each Paxos group has a long-lived **leader**. Leaders communicate with each\n",
        "other during multishard transactions.\n",
        "* **Every write** has to go through the Paxos group **leader**\n",
        "* **Reads** can be served directly from the tablet on up-to-date replicas. \n",
        "\n",
        "\n",
        "The leader holds a lock table that is used to implement concurrency control using the two-phase locking mechanism and a transaction manager that is responsible for multishard distributed transactions. \n",
        "\n",
        "Operations that require synchronization (such as writes and reads within a transaction) have to acquire the locks from the lock table, while other operations (snapshot reads) can access the data directly.\n",
        "\n",
        "For multishard transactions, group leaders have to coordinate and perform a twophase commit to ensure consistency, and use two-phase locking to ensure isolation.\n",
        "\n",
        "Since the 2PC algorithm requires the presence of all participants for a successful commit, it hurts availability. Spanner solves this by using Paxos groups rather than individual nodes as cohorts. This means that 2PC can continue operating even if some of the members of the group are down. \n",
        "\n",
        "\n",
        "Single-shard transactions do not have to consult the transaction manager.\n",
        "\n",
        "\n",
        "Spanner read-write transactions offer a serialization order called external consistency:\n",
        "* transaction timestamps reflect serialization order, even in cases of distributed transactions. \n",
        "* if transaction T1 commits before T2 starts, T1’s timestamp is smaller than the timestamp of T2.\n",
        "\n",
        "\n",
        "To summarize, Spanner uses Paxos for consistent transaction log replication, twophase commit for cross-shard transactions, and TrueTime for deterministic transaction ordering. This means that multipartition transactions have a higher cost due to an additional two-phase commit round, compared to Calvin."
      ],
      "metadata": {
        "id": "ov3oIMC9nU9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Calvin | Spanner\n",
        " ---|---\n",
        " global consensus | per shard consesus\n",
        " deterministic serial ordering | 2 phase commit\n",
        " cannot support SQL, noSQL only | SQL\n",
        " Fauna DB | YugaByte DB, Cockroach DB\n",
        " **read**\n",
        " high latency | low latency\n",
        " low throughput | high throughput\n",
        " **write**\n",
        " low latency for distributed transactions | low latency for single shard transactions\n",
        " high throughput for concurrent access for the same data | high throughput for concurrent access for random data\n",
        " **fault tolerance**\n",
        " leader with global consensus | fully decentralized architecture\n",
        " failure impacts all data | failure impacts subset of data\n",
        " **clock skew**\n",
        " no problem | needs true time\n",
        " **license**\n",
        " proprietary | open-source"
      ],
      "metadata": {
        "id": "097ikja1n3YA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Database Partitioning**\n",
        "\n",
        "\n",
        "Since storing all database records on a single node is rather unrealistic for the majority of modern applications, many databases use **partitioning**: a logical division of data into smaller manageable segments.\n",
        "\n",
        "\n",
        "\n",
        "**sharding**: every replica set acts as a single source for a subset (range) of data. Clients (or query coordinators) have to route requests based on the **routing key** to the correct replica.\n",
        "\n",
        "\n",
        "To use partitions most effectively, they have to be sized, taking the load and value distribution into consideration. \n",
        "\n",
        "\n",
        "\n",
        "When nodes are added to or removed from the cluster, the database has to repartition the data to maintain the balance. To ensure consistent movements, we\n",
        "should relocate the data before we update the cluster metadata and start routing\n",
        "requests to the new targets. Some databases perform **auto-sharding** and relocate the data using placement algorithms that determine optimal partitioning. These algorithms use information about read, write loads, and amounts of data in each shard.\n",
        "\n",
        "\n",
        "To find a target node from the routing key, some database systems compute a hash of the key, and use some form of mapping from the hash value to the node ID and taking a remainder of the division by the size of the cluster."
      ],
      "metadata": {
        "id": "Eu7Gfmyvrm95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Consistent Hashing**\n",
        "\n",
        "\n",
        "*hash(key) mod N* is not good if *N* changes, because a number of data should be relocated.\n",
        "\n",
        "Consistent hashing = calculate *hash(key) mod M*, where *M >> N* ans each server contains $m_i$ and handles data $m_i > \\text{hash}(\\text{key}) \\mod M > m_{i-1}$.\n",
        "\n",
        "![img](https://miro.medium.com/max/828/1*YUc0c0oOM-OzQDLHj48yYg.png)"
      ],
      "metadata": {
        "id": "uME5OeDH0DL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distributed Transactions with Percolator**\n",
        "\n",
        "\n",
        "If serializability is not required by the application, one of the ways to avoid the write anomalies is to use a transactional model called **snapshot isolation** (SI). \n",
        "\n",
        "\n",
        "**Snapshot isolation** guarantees that all reads made within the transaction are consistent with a **snapshot** of the database. The **snapshot** contains all values that were committed before the transaction’s start timestamp. If there’s a write-write conflict, only one of them will commit. This characteristic is usually referred to as first committer wins.\n",
        "\n",
        "* Snapshot isolation prevents **read skew**\n",
        "* It allows only repeatable reads of committed data.\n",
        "* Values are consistent, as they’re read from the snapshot at a specific timestamp.\n",
        "* Conflicting writes are aborted and retried to prevent inconsistencies.\n",
        "\n",
        "A **read skew** occurs when invariant between two variables is destroyed by concurrent transaction.\n",
        "\n",
        "$~$| $A_1$ | $A_2$ | $$A_1 + A_2 = 100$$\n",
        "---|---|---|---\n",
        "Intial state | 50\\$ | 50\\$ | True \n",
        "$T_1$: read balance | 50\\$ | 50\\$ | True\n",
        "$T_2$: move 20\\$ from $A_1$ to $A_2$ | 30\\$ | 70\\$ | True\n",
        "**T2 writes data when T1 is reading**\n",
        "$T_1$: read balance | 50\\$ | 70\\$ | False\n",
        "\n",
        "\n",
        "**BUT**\n",
        "* histories under snapshot isolation are not serializable. \n",
        "* we can still end up with a **write skew**\n",
        "\n",
        "A **write skew** occurs when each individual transaction respects the required invariants, but their combination does not satisfy these invariants.\n",
        "\n",
        "$~$| $A_1$ | $A_2$ | $$A_1 + A_2 > 0$$\n",
        "---|---|---|---\n",
        "Intial state | 100\\$ | 150\\$ | True \n",
        "$T_1$: withdraw 200\\$ from $A_1$ | -100\\$ | 150\\$ | True\n",
        "$T_2$: withdraw 200\\$ from $A_2$ | 100\\$ | -50\\$ | True\n",
        "**transactions write data concurrently**\n",
        "| -100\\$ | -50\\$ | False\n",
        "\n",
        "\n",
        "Snapshot isolation provides semantics that can be useful for many applications and has the major advantage of efficient reads, because no locks have to be acquired since snapshot data cannot be changed.\n",
        "\n",
        "\n",
        "**Percolator** is a library that implements a transactional API on top of the distributed database **Bigtable** (Wide Column Store). \n",
        "\n",
        "\n",
        "This is a great example of\n",
        "building a transaction API on top of the existing system. Percolator stores data\n",
        "records, committed data point locations (write metadata), and locks in different col‐\n",
        "umns. To avoid race conditions and reliably lock tables in a single RPC call, it uses a\n",
        "conditional mutation Bigtable API that allows it to perform read-modify-write operations with a single remote call.\n",
        "Each transaction has to consult the timestamp oracle (a source of cluster wide consistent monotonically increasing timestamps) twice: for a transaction start timestamp, and during commit. \n",
        "\n",
        "* Writes are buffered and committed using a client-driven two-phase commit.\n",
        "\n",
        "\n",
        "* Initial state. After the execution of the previous transaction, 1 is the latest timestamp for both accounts. No locks are held.\n",
        "\n",
        "Account | Timestamp | Data | Locks | Metadata\n",
        "--|--|--|---|---|\n",
        "1 | 1 | 100\\$ | - | timestamp 0 was the latest\n",
        "2 | 1 | 200\\$ | - | timestamp 0 was the latest\n",
        "\n",
        "* **Prewrite**. The transaction attempts to acquire locks for **all** cells written during the transaction. One of the locks is marked as primary and\n",
        "is used for client recovery. If any conflict is detected, the transaction\n",
        "aborts.\n",
        "\n",
        "Account | Timestamp | Data | Locks | Metadata\n",
        "--|--|--|---|---|\n",
        "1 | 1 | 100\\$ | Primary | timestamp 0 was the latest\n",
        "2 | 1 | 200\\$ | Primary at A1 | timestamp 0 was the latest\n",
        "\n",
        "\n",
        "* If all locks were successfully acquired and the possibility of conflict is ruled out, the transaction can continue. During the second phase, the client releases its locks, starting with the primary one. It publishes its write by replacing the lock with a write record, updating write metadata with the timestamp of the latest data point.\n",
        "\n",
        "Account | Timestamp | Data | Locks | Metadata\n",
        "--|--|--|---|---|\n",
        "1 | 2 | 150\\$ | - | timestamp 1 was the latest\n",
        "2 | 2 | 150\\$ | - | timestamp 1 was the latest\n",
        "\n",
        "\n",
        "Only one transaction can hold a lock at a time and all state transitions are\n",
        "atomic, so situations in which two transactions attempt to perform operations on the contents are not possible.\n",
        "\n",
        "\n",
        "One of the examples of databases based on the Percolator model is TiDB (“Ti” stands for Titatium). TiDB is a strongly consistent, highly available, and horizontally scalable open source database, compatible with MySQL."
      ],
      "metadata": {
        "id": "4sZ0yCGt1Wet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Coordination Avoidance**\n",
        "\n",
        "\n",
        "**Coordination** can be avoided, while preserving data integrity constraints, if operations are **invariant confluent**. Invariant Confluence (IConfluence) is defined as a property that ensures that two invariant-valid but diverged database states **can be merged** into a single valid, final state. Invariants in this case preserve consistency in ACID terms.\n",
        "\n",
        "\n",
        "Because any two valid states can be merged into a valid state, **I-Confluent** operations can be executed without additional coordination, which significantly improves performance characteristics and scalability potential.\n",
        "\n",
        "\n",
        "Transactions are executed against the local database versions (snapshots). If a transaction requires any state from other partitions for execution, this state is made available for it locally. If a transaction commits, resulting changes made to the local snapshot are migrated and merged with the snapshots on the other nodes. \n",
        "\n",
        "A system model that allows coordination avoidance has to guarantee the following properties:\n",
        "* **Global validity**\n",
        "Required invariants are always satisfied, for both merged and divergent committed database states, and transactions cannot observe invalid states.\n",
        "* **Availability**\n",
        "If all nodes holding states are reachable by the client, the transaction has to reach a commit decision, or abort, if committing it would violate one of the transaction invariants.\n",
        "* **Convergence**\n",
        "Nodes can maintain their local states independently, but in the absence of further transactions and indefinite network partitions, they have to be able to reach the same state.\n",
        "* **Coordination freedom**\n",
        "Local transaction execution is independent from the operations against the local\n",
        "states performed on behalf of the other nodes.\n",
        "\n",
        "\n",
        "One of the examples of implementing coordination avoidance is [Read-Atomic Multi\n",
        "Partition (RAMP)](https://people.eecs.berkeley.edu/~alig/papers/ramp.pdf) transactions. RAMP uses multiversion concurrency control and metadata of current in-flight operations to fetch any missing state updates from other nodes, allowing read and write operations to be executed concurrently. For example, readers that overlap with some writer modifying the same entry can be detected and, if necessary, repaired by retrieving required information from\n",
        "the in-flight write metadata in an additional round of communication.\n",
        "\n",
        "\n",
        "Using lock-based approaches in a distributed environment might be not the best idea, and instead of doing that, RAMP provides two properties:\n",
        "* **Synchronization independence**\n",
        "One client’s transactions won’t stall, abort, or force the other client’s transactions to wait.\n",
        "* **Partition independence**\n",
        "Clients do not have to contact partitions whose values aren’t involved in their\n",
        "transactions.\n",
        "\n",
        "RAMP introduces the read **atomic isolation level**: all or none transaction updates are visible to concurrent transactions (**fractured reads**: when a transaction observes only a subset of writes executed by some other transaction).\n",
        "\n",
        "RAMP offers atomic write visibility **without** requiring mutual exclusion = transactions can proceed without stalling each other. RAMP distributes transaction metadata that allows reads to detect concurrent inflight writes. By using this metadata, transactions can detect the presence of newer record versions, find and fetch the latest ones, and operate on them. \n",
        "\n",
        "To avoid coordination, all local commit decisions must also be valid globally. In RAMP, this is solved by requiring that, by the time a write becomes visible in one partition, writes from the same transaction in all other involved partitions are also visible for readers in those partitions.\n",
        "\n",
        "\n",
        "To allow readers and writers to proceed without blocking other concurrent readers and writers, while maintaining the read atomic isolation level both locally and system-wide (in all other partitions modified by the committing transaction), writes in RAMP are installed and made visible using **two-phase commit**.\n",
        "\n",
        "RAMP allows multiple versions of the same record to be present at any given\n",
        "moment: \n",
        "* latest value\n",
        "* in-flight uncommitted changes\n",
        "* stale versions, overwritten by later transactions (as soon as all concurrent readers complete, stale values can be discarded)\n"
      ],
      "metadata": {
        "id": "ygLkR4oL-azf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aY6E2RiRuLbq"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}